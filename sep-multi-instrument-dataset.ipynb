{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPp7IoRFzVByArV0zrYsi2T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/pouyahosseinzadeh/3f6b122fb4ba725e2dc5ed0e8cd37b87/sep-multi-instrument-dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "Bp--_CyYq-GX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cdflib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_Wp_n0LyEka",
        "outputId": "7a6e059d-f913-438c-f97d-39d0c186f342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cdflib\n",
            "  Downloading cdflib-1.3.6-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from cdflib) (2.0.2)\n",
            "Downloading cdflib-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cdflib\n",
            "Successfully installed cdflib-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tynZ07hRywrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vx"
      ],
      "metadata": {
        "id": "z7WEDL5xC_0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from datetime import timedelta\n",
        "from tqdm import tqdm\n",
        "import cdflib\n",
        "\n",
        "# ==========================\n",
        "# CONFIG / PATHS\n",
        "# ==========================\n",
        "EVENTS_CSV = Path(\"/content/sample_data/1998_2013_MEMSEP_dataset.csv\")\n",
        "OUT_CSV    = Path(\"/content/1998_2013_MEMSEP_dataset_with_Vx.csv\")  # final output here\n",
        "CACHE_DIR  = Path(\"/content/_omni_hro2_cache\")\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "BASE_URL = \"https://cdaweb.gsfc.nasa.gov/pub/data/omni/omni_cdaweb/hro2_5min\"\n",
        "\n",
        "FREQ = \"5min\"\n",
        "WINDOW_MIN = 24 * 60\n",
        "NUM_STEPS = WINDOW_MIN // 5  # 288\n",
        "COLS = [f\"Vx_{i}\" for i in range(NUM_STEPS, 0, -1)]  # Vx_288 ... Vx_1\n",
        "\n",
        "# Candidate names for Vx\n",
        "VX_SCALARS = [\n",
        "    \"Vx\", \"VX\", \"VX_GSE\", \"VX_GSM\", \"V_GSE_X\", \"V_GSM_X\", \"V_X_GSE\", \"V_X_GSM\",\n",
        "    \"Vx_gse\", \"Vx_gsm\"\n",
        "]\n",
        "VX_VECTORS = [\n",
        "    \"V_GSE\", \"V_GSM\", \"VGSE\", \"VGSM\", \"PLASMA_V_GSE\", \"PLASMA_V_GSM\"\n",
        "]\n",
        "\n",
        "def http_get(url, timeout=60):\n",
        "    r = requests.get(url, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r\n",
        "\n",
        "def latest_month_cdf_url(year: int, month: int) -> str:\n",
        "    \"\"\"Pick the latest version omni_hro2_5min_YYYYMM01_vNN.cdf in the year dir.\"\"\"\n",
        "    ydir = f\"{BASE_URL}/{year:04d}/\"\n",
        "    html = http_get(ydir).text\n",
        "    tag = f\"{year:04d}{month:02d}01\"\n",
        "    pat = re.compile(rf\"omni_hro2_5min_{tag}_v(\\d+)\\.cdf\", re.I)\n",
        "    versions = [int(v) for v in pat.findall(html)]\n",
        "    if not versions:\n",
        "        raise FileNotFoundError(f\"No hro2_5min CDF for {tag} under {ydir}\")\n",
        "    vmax = max(versions)\n",
        "    return f\"{ydir}omni_hro2_5min_{tag}_v{vmax:02d}.cdf\"\n",
        "\n",
        "def ensure_month_cdf(year: int, month: int) -> Path:\n",
        "    \"\"\"Download month CDF into cache.\"\"\"\n",
        "    url = latest_month_cdf_url(year, month)\n",
        "    local = CACHE_DIR / url.split(\"/\")[-1]\n",
        "    if not local.exists():\n",
        "        data = http_get(url).content\n",
        "        with open(local, \"wb\") as f:\n",
        "            f.write(data)\n",
        "    return local\n",
        "\n",
        "def get_zvars(info) -> list:\n",
        "    \"\"\"Support both dict- and object-style cdflib CDFInfo.\"\"\"\n",
        "    try:\n",
        "        # dict-like\n",
        "        return list(info[\"zVariables\"])\n",
        "    except Exception:\n",
        "        # object-like (attributes)\n",
        "        z = getattr(info, \"zVariables\", None)\n",
        "        if z is None:\n",
        "            raise TypeError(\"cdf_info() has no zVariables field\")\n",
        "        return list(z)\n",
        "\n",
        "def read_vx_from_cdf(cdf_path: Path) -> pd.DataFrame:\n",
        "    \"\"\"Return DataFrame with DatetimeIndex and one column 'Vx'.\"\"\"\n",
        "    cdf = cdflib.CDF(str(cdf_path))\n",
        "    info = cdf.cdf_info()\n",
        "    zvars = get_zvars(info)\n",
        "\n",
        "    # time variable\n",
        "    epoch_var = None\n",
        "    for cand in zvars:\n",
        "        if \"epoch\" in cand.lower():\n",
        "            epoch_var = cand\n",
        "            break\n",
        "    if epoch_var is None:\n",
        "        raise KeyError(f\"No Epoch var in {cdf_path.name}\")\n",
        "\n",
        "    times = cdflib.cdfepoch.to_datetime(cdf.varget(epoch_var))\n",
        "    times = pd.to_datetime(times, utc=True).tz_convert(None)\n",
        "\n",
        "    # scalar names first\n",
        "    vx_series = None\n",
        "    for name in VX_SCALARS:\n",
        "        if name in zvars:\n",
        "            arr = cdf.varget(name)\n",
        "            vx_series = pd.Series(arr, index=times, name=\"Vx\").astype(float)\n",
        "            used = name\n",
        "            break\n",
        "\n",
        "    # vector names, take X component [:,0]\n",
        "    if vx_series is None:\n",
        "        for name in VX_VECTORS:\n",
        "            if name in zvars:\n",
        "                arr = cdf.varget(name)\n",
        "                if hasattr(arr, \"ndim\") and arr.ndim == 2 and arr.shape[1] >= 1:\n",
        "                    arr = arr[:, 0]\n",
        "                vx_series = pd.Series(arr, index=times, name=\"Vx\").astype(float)\n",
        "                used = f\"{name}[:,0]\"\n",
        "                break\n",
        "\n",
        "    if vx_series is None:\n",
        "        # last resort: any 2D V*GSE/GSM var\n",
        "        candidates = [v for v in zvars if (\"V\" in v.upper() and (\"GSE\" in v.upper() or \"GSM\" in v.upper()))]\n",
        "        for name in candidates:\n",
        "            arr = cdf.varget(name)\n",
        "            if hasattr(arr, \"ndim\") and arr.ndim == 2 and arr.shape[1] >= 1:\n",
        "                arr = arr[:, 0]\n",
        "                vx_series = pd.Series(arr, index=times, name=\"Vx\").astype(float)\n",
        "                used = f\"{name}[:,0]\"\n",
        "                break\n",
        "\n",
        "    if vx_series is None:\n",
        "        raise KeyError(f\"No usable Vx var in {cdf_path.name}; first vars: {zvars[:12]}\")\n",
        "\n",
        "    # Clean fills, sort, dedup\n",
        "    vx_series.replace([99999.8984375, 99999.9, 99999, 9999, 1e31, -1e31, 1e30, -1e30], np.nan, inplace=True)\n",
        "    # after the .replace(...):\n",
        "    vx_series[np.isclose(vx_series, 99999.9, atol=2)] = np.nan\n",
        "    vx_series[vx_series.abs() > 5000] = np.nan\n",
        "    vx_series = vx_series[~vx_series.index.duplicated(keep=\"last\")].sort_index()\n",
        "\n",
        "    # Snap timestamps to exact 5-min bins\n",
        "    snapped = (vx_series.index.view(\"int64\") // (5*60*1_000_000_000)) * (5*60*1_000_000_000)\n",
        "    vx_series.index = pd.to_datetime(snapped)\n",
        "\n",
        "    return vx_series.to_frame()  # col 'Vx'\n",
        "\n",
        "def months_for_window(onset: pd.Timestamp):\n",
        "    prev = (onset - pd.Timedelta(days=1))\n",
        "    return {(prev.year, prev.month), (onset.year, onset.month)}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_window(month_dfs: list, onset: pd.Timestamp) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Build a 5-min grid for the 24h BEFORE onset,\n",
        "    snapping onset up to the next 5-min boundary (ceil).\n",
        "    Last sample is strictly < onset (i.e., end_aligned - 5 min).\n",
        "    \"\"\"\n",
        "    NUM_STEPS = 288  # 24h @ 5min\n",
        "    FREQ = \"5min\"\n",
        "\n",
        "    # 1) snap onset to next 5-min boundary\n",
        "    end_aligned = pd.Timestamp(onset).ceil(\"5min\")  # e.g., 03:21 -> 03:25\n",
        "\n",
        "    # 2) exact 5-min index: [end_aligned-24h, end_aligned-5min]\n",
        "    idx = pd.date_range(end=end_aligned - pd.Timedelta(minutes=5),\n",
        "                        periods=NUM_STEPS, freq=FREQ)\n",
        "\n",
        "    # 3) combine months and snap their timestamps to 5-min bins\n",
        "    df_all = pd.concat(month_dfs, ignore_index=False).sort_index()\n",
        "    df_all = df_all[~df_all.index.duplicated(keep=\"last\")]\n",
        "\n",
        "    # snap source to exact 5-min bins (handles small clock skews)\n",
        "    snapped_ns = (df_all.index.view(\"int64\") // (5*60*1_000_000_000)) * (5*60*1_000_000_000)\n",
        "    df_all.index = pd.to_datetime(snapped_ns)\n",
        "\n",
        "    # 4) align + fill (dataset style)\n",
        "    s = df_all.reindex(idx)[\"Vx\"]\n",
        "    s = s.ffill().bfill().fillna(-9999.0)\n",
        "\n",
        "    return s.to_numpy()\n",
        "\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# MAIN\n",
        "# ==========================\n",
        "df = pd.read_csv(EVENTS_CSV)\n",
        "if \"FlrOnset\" not in df.columns:\n",
        "    raise SystemExit(\"FlrOnset column missing in your CSV.\")\n",
        "df[\"FlrOnset\"] = pd.to_datetime(df[\"FlrOnset\"], utc=True).dt.tz_convert(None)\n",
        "\n",
        "# Prefetch month files\n",
        "needed = set()\n",
        "for t in df[\"FlrOnset\"]:\n",
        "    needed |= months_for_window(t)\n",
        "\n",
        "month_cache = {}\n",
        "print(\"Downloading & parsing OMNI hro2_5min CDFs …\")\n",
        "for (y, m) in tqdm(sorted(needed)):\n",
        "    try:\n",
        "        cdf_path = ensure_month_cdf(y, m)\n",
        "        month_cache[(y, m)] = read_vx_from_cdf(cdf_path)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] {y}-{m:02d}: {e}\")\n",
        "        month_cache[(y, m)] = pd.DataFrame({\"Vx\": []})\n",
        "\n",
        "# Build Vx windows\n",
        "vx_mat = np.full((len(df), NUM_STEPS), -9999.0, dtype=float)\n",
        "\n",
        "print(\"Building 24h Vx windows before FlrOnset …\")\n",
        "for i, onset in enumerate(tqdm(df[\"FlrOnset\"].tolist())):\n",
        "    months = months_for_window(onset)\n",
        "    dfs = [month_cache.get(k, pd.DataFrame({\"Vx\": []})) for k in months]\n",
        "    try:\n",
        "        vals = build_window(dfs, onset)\n",
        "        vx_mat[i, :] = vals\n",
        "    except Exception:\n",
        "        # keep -9999 row\n",
        "        pass\n",
        "\n",
        "# Attach to end of CSV, preserve everything\n",
        "vx_df = pd.DataFrame(vx_mat, columns=COLS)\n",
        "df_out = pd.concat([df, vx_df], axis=1)\n",
        "df_out.to_csv(OUT_CSV, index=False)\n",
        "print(f\"✓ Saved: {OUT_CSV}\")\n",
        "\n",
        "all_missing = int((vx_df == -9999.0).all(axis=1).sum())\n",
        "print(f\"[SUMMARY] Rows with all -9999: {all_missing} / {len(df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AciT5YHnyEiU",
        "outputId": "6379f08f-64e7-485d-fcf6-349da75ed889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading & parsing OMNI hro2_5min CDFs …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 163/163 [02:35<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building 24h Vx windows before FlrOnset …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17794/17794 [01:34<00:00, 187.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved: /content/1998_2013_MEMSEP_dataset_with_Vx.csv\n",
            "[SUMMARY] Rows with all -9999: 84 / 17794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, numpy as np, pandas as pd\n",
        "\n",
        "IN  = \"/content/1998_2013_MEMSEP_dataset_with_Vx.csv\"\n",
        "OUT = \"/content/1998_2013_MEMSEP_dataset_with_Vx_filled.csv\"\n",
        "\n",
        "df = pd.read_csv(IN)\n",
        "\n",
        "# grab Vx_288 … Vx_1\n",
        "vx_cols = [c for c in df.columns if re.fullmatch(r\"Vx_\\d+\", c)]\n",
        "vx_cols = sorted(vx_cols, key=lambda s: int(s.split(\"_\")[1]), reverse=True)  # 288→1\n",
        "\n",
        "# to numeric; non-numeric -> NaN\n",
        "X = df[vx_cols].apply(pd.to_numeric, errors=\"coerce\").to_numpy(dtype=float)\n",
        "\n",
        "# treat ALL common sentinels + absurd values as missing\n",
        "sentinels = { 999.989990234375, -9999, 99999.8984375, 99999.9, 99999, 9999, 1e31, -1e31, 1e30, -1e30 }\n",
        "mask_bad = np.isin(X, list(sentinels)) | np.isclose(X, 99999.9, atol=2) | (~np.isfinite(X)) | (np.abs(X) > 5000)\n",
        "X[mask_bad] = np.nan\n",
        "\n",
        "# interpolate each row across the 288 points\n",
        "idx = np.arange(X.shape[1])\n",
        "for i in range(X.shape[0]):\n",
        "    m = ~np.isnan(X[i])\n",
        "    if m.sum() >= 2:\n",
        "        X[i, ~m] = np.interp(idx[~m], idx[m], X[i, m])  # linear; fills ends by edge values\n",
        "    elif m.sum() == 1:\n",
        "        X[i, :] = X[i, m][0]  # only one real point (flat fill)\n",
        "\n",
        "# any remaining all-NaN rows --- fill with column medians, then global median as last resort\n",
        "col_med = np.nanmedian(X, axis=0)\n",
        "nan_rows, nan_cols = np.where(np.isnan(X))\n",
        "X[nan_rows, nan_cols] = col_med[nan_cols]\n",
        "gmed = np.nanmedian(X)\n",
        "X[np.isnan(X)] = gmed\n",
        "\n",
        "df[vx_cols] = X\n",
        "df.to_csv(OUT, index=False)\n",
        "print(\"Saved:\", OUT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt1zkXhQyci0",
        "outputId": "28525ac0-a0cf-4fc3-b4df-618c1050d6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/1998_2013_MEMSEP_dataset_with_Vx_filled.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8B7lzn0opMjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# >10 MeV"
      ],
      "metadata": {
        "id": "eMC-yC07kZED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "os.makedirs(\"/content/sample_data/figs_gt10\", exist_ok=True)\n",
        "\n",
        "# files\n",
        "variables = {\n",
        "    \"Vx\": \"/content/sample_data/1998_2013_MEMSEP_dataset_with_Vx_filled.csv\",\n",
        "    \"FlowSpeed\": \"/content/sample_data/1998_2013_MEMSEP_dataset_with_flow_speed_filled.csv\",\n",
        "    \"ProtonDensity\": \"/content/sample_data/1998_2013_MEMSEP_dataset_with_proton_density.csv\",\n",
        "    \"ProtonTemp\": \"/content/sample_data/1998_2013_MEMSEP_dataset_with_T_filled.csv\",\n",
        "    \"F\": \"/content/sample_data/1998_2013_MEMSEP_dataset_with_F_filled.csv\",\n",
        "    \"Xs\": \"/content/sample_data/1998_2013_MEMSEP_dataset_with_Xs_filled.csv\",\n",
        "    \"Xl\": \"/content/sample_data/1998_2013_MEMSEP_dataset_with_Xl_filled.csv\",\n",
        "    \"P4\": \"/content/sample_data/1998_2013_MEMSEP_dataset_with_P4_filled.csv\",\n",
        "    \"P5\": \"/content/sample_data/1998_2013_MEMSEP_dataset_with_P5_filled.csv\",\n",
        "    \"P6\": \"/content/sample_data/1998_2013_MEMSEP_dataset_with_P6_filled.csv\",\n",
        "}\n",
        "\n",
        "SENTINELS = { -9999, 99999.8984375, 99999.9, 99999, 9999, 1e31, -1e31, 1e30, -1e30 }\n",
        "THRESH_10PFU = 10.0  # >10 MeV NOAA threshold at sep_peak_2\n",
        "\n",
        "def pick_timeseries_block(df: pd.DataFrame) -> list[str]:\n",
        "    pat = re.compile(r\"^(?P<prefix>.+)_(?P<idx>\\d+)$\")\n",
        "    groups = {}\n",
        "    for c in df.columns:\n",
        "        m = pat.match(str(c))\n",
        "        if not m: continue\n",
        "        p = m.group(\"prefix\"); k = int(m.group(\"idx\"))\n",
        "        groups.setdefault(p, []).append((k, c))\n",
        "    if not groups: return []\n",
        "    def score(items):\n",
        "        idxs = [k for k,_ in items]\n",
        "        return (len(items), max(idxs)-min(idxs))\n",
        "    pref = max(groups.items(), key=lambda kv: score(kv[1]))[0]\n",
        "    items = sorted(groups[pref], key=lambda t: t[0], reverse=True)\n",
        "    seen, cols = set(), []\n",
        "    for k, c in items:\n",
        "        if k not in seen:\n",
        "            cols.append(c); seen.add(k)\n",
        "    if len(cols) >= 288: cols = cols[:288]\n",
        "    return cols\n",
        "\n",
        "def sanitize_numeric(frame: pd.DataFrame) -> np.ndarray:\n",
        "    X = frame.apply(pd.to_numeric, errors=\"coerce\").to_numpy(dtype=float)\n",
        "    mask_bad = np.isin(X, list(SENTINELS)) | np.isclose(X, 99999.9, atol=2) | (~np.isfinite(X))\n",
        "    X[mask_bad] = np.nan\n",
        "    return X\n",
        "\n",
        "def mean_sem_ci(X: np.ndarray):\n",
        "    m = np.nanmean(X, axis=0)\n",
        "    s = np.nanstd(X, axis=0, ddof=1)\n",
        "    n = np.sum(~np.isnan(X), axis=0).astype(float)\n",
        "    sem = s / np.sqrt(np.maximum(n, 1))\n",
        "    z = 1.959963984540054\n",
        "    ci = z * sem\n",
        "    return m, sem, ci, n\n",
        "\n",
        "def contiguous_regions(mask: np.ndarray):\n",
        "    if mask.size == 0: return\n",
        "    in_region = False; start = 0\n",
        "    for i, v in enumerate(mask):\n",
        "        if v and not in_region: in_region = True; start = i\n",
        "        elif not v and in_region: in_region = False; yield (start, i-1)\n",
        "    if in_region: yield (start, len(mask)-1)\n",
        "\n",
        "def build_gt10_sep_vs_nsep(path: str):\n",
        "    df = pd.read_csv(path)\n",
        "    if \"event_type\" not in df.columns:\n",
        "        raise ValueError(f\"'event_type' column missing in {path}\")\n",
        "    df = df.drop(columns=[c for c in df.columns if str(c).lower().startswith(\"unnamed\")], errors=\"ignore\")\n",
        "\n",
        "    # time-series block\n",
        "    cols = pick_timeseries_block(df)\n",
        "    if not cols:\n",
        "        raise ValueError(f\"No time-series block found in {path}\")\n",
        "\n",
        "    # >10 MeV mask: event_type==1 & sep_peak_2 >= THRESH_10PFU\n",
        "    if \"sep_peak_2\" not in df.columns:\n",
        "        raise ValueError(f\"'sep_peak_2' missing in {path} (needed for >10 MeV selection)\")\n",
        "    peak10 = pd.to_numeric(df[\"sep_peak_2\"], errors=\"coerce\")\n",
        "    gt10_mask = (df[\"event_type\"] == 1) & (peak10 >= THRESH_10PFU)\n",
        "\n",
        "    SEP_gt10  = sanitize_numeric(df.loc[gt10_mask, cols])           # SEP (>10 MeV subset)\n",
        "    NSEP      = sanitize_numeric(df.loc[df[\"event_type\"] == 0, cols])  # all NSEPs\n",
        "\n",
        "    return cols, SEP_gt10, NSEP, int(gt10_mask.sum()), int((df[\"event_type\"]==0).sum())\n",
        "\n",
        "def plot_curves_with_ci(varname, cols, SEP, NSEP, outdir):\n",
        "    T = len(cols); x_hours = np.linspace(24, 0, num=T)\n",
        "    m_s, _, ci_s, n_s = mean_sem_ci(SEP)\n",
        "    m_n, _, ci_n, n_n = mean_sem_ci(NSEP)\n",
        "\n",
        "    pvals = np.full(T, np.nan)\n",
        "    for j in range(T):\n",
        "        a, b = SEP[:, j], NSEP[:, j]\n",
        "        _, p = ttest_ind(a, b, equal_var=False, nan_policy=\"omit\"); pvals[j] = p\n",
        "    sig = (pvals < 0.05)\n",
        "\n",
        "    plt.figure(figsize=(8, 4.5))\n",
        "    for a, b in contiguous_regions(sig):\n",
        "        xa, xb = x_hours[a], x_hours[b]\n",
        "        plt.axvspan(xb, xa, color=\"0.9\", zorder=0)\n",
        "\n",
        "    plt.plot(x_hours, m_s, lw=2.0, label=f\"SEP >10 MeV (n={int(np.nanmax(n_s))})\")\n",
        "    plt.fill_between(x_hours, m_s-ci_s, m_s+ci_s, alpha=0.25, linewidth=0)\n",
        "\n",
        "    plt.plot(x_hours, m_n, lw=2.0, label=f\"NSEP (n={int(np.nanmax(n_n))})\")\n",
        "    plt.fill_between(x_hours, m_n-ci_n, m_n+ci_n, alpha=0.25, linewidth=0)\n",
        "\n",
        "    plt.title(f\"{varname}: Mean ±95% CI (shaded: p<0.05)\")\n",
        "    plt.xlabel(\"Hours Before Flare\"); plt.ylabel(varname)\n",
        "    plt.xlim(24, 0); plt.axvline(0, color=\"k\", ls=\"--\", lw=1)\n",
        "    plt.grid(True, ls=\"--\", alpha=0.4); plt.legend(frameon=False)\n",
        "    plt.tight_layout()\n",
        "    base = os.path.join(outdir, f\"curves_{varname}_gt10\")\n",
        "    plt.savefig(base + \".pdf\"); plt.savefig(base + \".png\", dpi=300); plt.close()\n",
        "    print(f\"Saved: {base}.pdf/.png\")\n",
        "\n",
        "def plot_effectsize_heatmap_rownorm(effects, outdir):\n",
        "    varnames = list(effects.keys())\n",
        "    D = np.vstack([effects[v][\"diff\"] for v in varnames])\n",
        "    P = np.vstack([effects[v][\"p\"] for v in varnames])\n",
        "    Dn = (D - np.nanmean(D, axis=1, keepdims=True)) / (np.nanstd(D, axis=1, ddof=1, keepdims=True) + 1e-9)\n",
        "    x_hours = np.linspace(24, 0, D.shape[1])\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    im = plt.imshow(Dn, aspect=\"auto\", cmap=\"coolwarm\",\n",
        "                    extent=[24, 0, 0, len(varnames)], vmin=-2.5, vmax=2.5)\n",
        "    cb = plt.colorbar(im, label=\"Row-normalized effect\")\n",
        "\n",
        "    for i in range(len(varnames)):\n",
        "        sig = P[i] < 0.05\n",
        "        xs = x_hours[sig]; ys = np.full(xs.shape, i + 0.5)\n",
        "        plt.scatter(xs, ys, marker=\"|\", s=30, c=\"k\")\n",
        "\n",
        "    plt.yticks(np.arange(0.5, len(varnames)+0.5), varnames)\n",
        "    plt.xlabel(\"Hours Before Flare\"); plt.ylabel(\"Variable\")\n",
        "    plt.title(\"Row-normalized Effect: SEP (>10 MeV) − NSEP (markers: p<0.05)\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    base = os.path.join(outdir, \"effectsize_heatmap_rownorm_gt10\")\n",
        "    plt.savefig(base + \".pdf\"); plt.savefig(base + \".png\", dpi=300); plt.close()\n",
        "    print(f\"Saved: {base}.pdf/.png\")\n",
        "\n",
        "def make_summary_key_times(varname, cols, SEP, NSEP, hours=[24,12,6,0]):\n",
        "    T = len(cols); x_hours = np.linspace(24, 0, num=T)\n",
        "    rows = []\n",
        "    for h in hours:\n",
        "        j = np.argmin(np.abs(x_hours - h))\n",
        "        a, b = SEP[:, j], NSEP[:, j]\n",
        "        mean_sep, mean_nsp = np.nanmean(a), np.nanmean(b)\n",
        "        diff = mean_sep - mean_nsp\n",
        "        _, p = ttest_ind(a, b, equal_var=False, nan_policy=\"omit\")\n",
        "        rows.append({\"variable\": varname, \"hours_before\": float(x_hours[j]),\n",
        "                     \"sep_gt10_mean\": mean_sep, \"nsep_mean\": mean_nsp,\n",
        "                     \"difference\": diff, \"p_value\": p})\n",
        "    return rows\n",
        "\n",
        "# ---------- RUN ----------\n",
        "effects = {}\n",
        "summary_rows = []\n",
        "outdir = \"/content/sample_data/figs_gt10\"\n",
        "\n",
        "for var, path in variables.items():\n",
        "    print(f\"\\n=== {var} ===\")\n",
        "    try:\n",
        "        cols, SEP_gt10, NSEP, n_gt10, n_nsep = build_gt10_sep_vs_nsep(path)\n",
        "        print(f\"Counts -> SEP >10 MeV: {n_gt10}, NSEP: {n_nsep}\")\n",
        "        if (SEP_gt10.size == 0) or (NSEP.size == 0):\n",
        "            print(\"[WARN] Empty group(s); skipping plots.\"); continue\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] {var}: {e}\"); continue\n",
        "\n",
        "    # curves\n",
        "    plot_curves_with_ci(var, cols, SEP_gt10, NSEP, outdir)\n",
        "\n",
        "    # effects for heatmap\n",
        "    diffs, pvals = [], []\n",
        "    for j in range(len(cols)):\n",
        "        a, b = SEP_gt10[:, j], NSEP[:, j]\n",
        "        diffs.append(np.nanmean(a) - np.nanmean(b))\n",
        "        _, p = ttest_ind(a, b, equal_var=False, nan_policy=\"omit\")\n",
        "        pvals.append(p)\n",
        "    effects[var] = {\"diff\": np.array(diffs), \"p\": np.array(pvals)}\n",
        "\n",
        "    # summary rows\n",
        "    summary_rows += make_summary_key_times(var, cols, SEP_gt10, NSEP, hours=[24,12,6,0])\n",
        "\n",
        "# heatmap\n",
        "if effects:\n",
        "    plot_effectsize_heatmap_rownorm(effects, outdir)\n",
        "\n",
        "# summary CSV\n",
        "if summary_rows:\n",
        "    df_sum = pd.DataFrame(summary_rows)\n",
        "    out_csv = os.path.join(outdir, \"summary_key_times_gt10.csv\")\n",
        "    df_sum.to_csv(out_csv, index=False)\n",
        "    print(f\"Saved: {out_csv}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHz9O9sRkY0l",
        "outputId": "07ffe800-e357-4d26-d0f7-1f865ba098b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Vx ===\n",
            "Counts -> SEP >10 MeV: 168, NSEP: 17542\n",
            "Saved: /content/sample_data/figs_gt10/curves_Vx_gt10.pdf/.png\n",
            "\n",
            "=== FlowSpeed ===\n",
            "Counts -> SEP >10 MeV: 168, NSEP: 17542\n",
            "Saved: /content/sample_data/figs_gt10/curves_FlowSpeed_gt10.pdf/.png\n",
            "\n",
            "=== ProtonDensity ===\n",
            "Counts -> SEP >10 MeV: 168, NSEP: 17542\n",
            "Saved: /content/sample_data/figs_gt10/curves_ProtonDensity_gt10.pdf/.png\n",
            "\n",
            "=== ProtonTemp ===\n",
            "Counts -> SEP >10 MeV: 168, NSEP: 17542\n",
            "Saved: /content/sample_data/figs_gt10/curves_ProtonTemp_gt10.pdf/.png\n",
            "\n",
            "=== F ===\n",
            "Counts -> SEP >10 MeV: 168, NSEP: 17542\n",
            "Saved: /content/sample_data/figs_gt10/curves_F_gt10.pdf/.png\n",
            "\n",
            "=== Xs ===\n",
            "Counts -> SEP >10 MeV: 168, NSEP: 17542\n",
            "Saved: /content/sample_data/figs_gt10/curves_Xs_gt10.pdf/.png\n",
            "\n",
            "=== Xl ===\n",
            "Counts -> SEP >10 MeV: 168, NSEP: 17542\n",
            "Saved: /content/sample_data/figs_gt10/curves_Xl_gt10.pdf/.png\n",
            "\n",
            "=== P4 ===\n",
            "Counts -> SEP >10 MeV: 168, NSEP: 17542\n",
            "Saved: /content/sample_data/figs_gt10/curves_P4_gt10.pdf/.png\n",
            "\n",
            "=== P5 ===\n",
            "Counts -> SEP >10 MeV: 168, NSEP: 17542\n",
            "Saved: /content/sample_data/figs_gt10/curves_P5_gt10.pdf/.png\n",
            "\n",
            "=== P6 ===\n",
            "Counts -> SEP >10 MeV: 168, NSEP: 17542\n",
            "Saved: /content/sample_data/figs_gt10/curves_P6_gt10.pdf/.png\n",
            "Saved: /content/sample_data/figs_gt10/effectsize_heatmap_rownorm_gt10.pdf/.png\n",
            "Saved: /content/sample_data/figs_gt10/summary_key_times_gt10.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hit4ggLrY8dy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}